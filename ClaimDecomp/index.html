<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CLAIMDECOMP</title>

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link href="./css/style.css" rel="stylesheet">
    <!-- Favicons -->
  </head>
  <body>
    <nav id="navbar" class="shadow-sm navbar navbar-light bg-light">
      <div id="navbar-content" class="container">
        <a class="navbar-brand" href="#">CLAIMDECOMP</a>
        <ul class="nav nav-pills">
          <li class="nav-item"><a href="#" class="nav-link active" aria-current="page">Home</a></li>
          <li class="nav-item"><a href="./explorer.html" class="nav-link">Data Explorer</a></li>
        </ul>
      </div>
    </nav>
    <main id="main" class="container fs-5">
      <div class="p-5">
        <h1 class="text-center display-5 fw-bold">Generating Literal and Implied Subquestions to Fact-check Complex Claims</h1>
      </div>
      <div class="pt-3">
        <h2 class="display-6 fw-bold">About</h2>
        <p>
        Verifying complex political claims, where politicians use various tactics for their agenda, is critical yet difficult. The performance of automatic fact-checking systems is still limited, and a prediction like ``half-true'' alone is not very useful, since we have no idea which parts of the claim are true and which are not. In this work, we focus on decomposing a complex claim into several yes-no subquestions whose answer influences the veracity of the claim. We collect CLAIMDECOMP, a dataset of decompositions for over 1000 claims, generated by trained annotators who are provided with the claim and a verification paragraph written by professional fact-checkers. Our comprehensive annotation addresses both explicit propositions of the original claim and its implicit facets, additional context that would soften the claim, or more generally other unstated factors. We study whether state-of-the-art question generation models can generate such subquestions, showing that these models generate reasonable questions to ask, but predicting the comprehensive set of subquestions from the original claim without evidence remains challenging. We further show that these subquestions can help with identifying relevant evidence to fact-check the full claim and composing the veracity through the answers to these questions, suggesting that they can be useful pieces of a fact-checking pipeline.
        </p>
        <div>
          <a class="btn btn-primary btn-lg" type="button" href="https://arxiv.org/pdf/2205.06938.pdf">Read the Paper</a>
          <a class="btn btn-success btn-lg" type="button" href="https://github.com/jifan-chen/subquestions-for-fact-checking">Download the Data & Code</a>
<!--          <a class="btn btn-danger btn-lg" type="button" href="#">Read the Datasheet</a>-->
        </div>
      </div>
      <div class="pt-5">
        <h2 class="display-6 fw-bold">Citations</h2>
        <p>If you find our work helpful, please cite us.</p>
        <pre class="citations border bg-light p-2 fs-6">
@article{chen-etal-2022-generating,
  title={Generating Literal and Implied Subquestions to Fact-check Complex Claims},
  author={Chen, Jifan and Sriram, Aniruddh and Choi, Eunsol and Durrett, Greg},
  journal={arXiv preprint},
  year={2022}
}</pre>

      </div>
      <div class="pt-4">
        <h2 class="display-6 fw-bold">Contact</h2>
        <p>
        If you have any questions, please contact <a href="https://jifan-chen.github.io/"> Jifan Chen </a>: jfchen@cs.utexas.edu
        </p>
      </div>
    </main>
    <!-- Bootstrap Bundle -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
  </body>
</html>
